backend: XPU
cpp_namespace: at
use_out_as_primary: true
device_guard: true
supported:
  - add.Tensor
  - add_.Tensor
  - add.out
  - add.Scalar
  - add_.Scalar
  - add.Scalar_out
<<<<<<< HEAD
  - add.Scalar #Composite
  - add_.Scalar #Composite
  - add.Scalar_out #Composite
  - _adaptive_avg_pool2d_backward #need add
=======
  - _adaptive_avg_pool2d_backward
  - adaptive_avg_pool2d.out
  - _adaptive_avg_pool2d
>>>>>>> main
  - cumsum
  - cumsum.out
  - cumsum_
  - cumprod
  - cumprod.out
  - cumprod_
  - sub.Tensor
  - sub_.Tensor
  - sub.out
  - mul.Tensor
  - mul_.Tensor
  - mul.out
  - mul.Scalar
  - mul_.Scalar
  - mul.Scalar_out
  - div.Tensor
  - div_.Tensor
  - div.out
  - div.Scalar # COMPO
  - div_.Scalar #COMPO
  - div.Scalar_out #COMPO
  - div.Tensor_mode
  - div_.Tensor_mode
  - div.out_mode
  - rsub.Tensor
  - rsub.Tensor_out
  - rsub.Scalar #composiite
  - rsub.Scalar_out #composite
  - remainder.Tensor
  - remainder_.Tensor
  - remainder.Tensor_out
  - remainder.Scalar #composite
  - remainder_.Scalar #composite
  - remainder.Scalar_out #composite
  - remainder.Scalar_Tensor #composite
  - remainder.Scalar_Tensor_out #composite
  - fmod.Tensor
  - fmod_.Tensor
  - fmod.Tensor_out
  - fmod.Scalar #composite
  - fmod_.Scalar #composie
  - fmod.Scalar_out #composite
  - tanh_backward
  - tanh_backward.grad_input
  - eq.Scalar #structured delegate
  - eq.Scalar_out #structured delgeat
  - eq_.Scalar #structured delgeat
  - eq.Tensor
  - eq.Tensor_out
  - eq_.Tensor #structured delgeat
  - ne.Scalar #structured delgeat
  - ne.Scalar_out #structured delgeat
  - ne_.Scalar #structured delgeat
  - ne.Tensor
  - ne.Tensor_out
  - ne_.Tensor #structured delgeat
  - lt.Scalar #structured
  - lt.Scalar_out #structured
  - lt_.Scalar #structured
  - lt.Tensor
  - lt.Tensor_out 
  - lt_.Tensor #structured
  - le.Scalar #structured
  - le.Scalar_out #structured
  - le_.Scalar #structured
  - le.Tensor
  - le.Tensor_out
<<<<<<< HEAD
  - le_.Tensor #structured
  - gt.Scalar #structured
  - gt.Scalar_out #structured
  - gt_.Scalar #structured
=======
  - le_.Tensor
  - lerp.Tensor
  - lerp.Tensor_out
  - lerp_.Tensor
  - lerp.Scalar
  - lerp.Scalar_out
  - lerp_.Scalar
  - gt.Scalar
  - gt.Scalar_out
  - gt_.Scalar
>>>>>>> main
  - gt.Tensor
  - gt.Tensor_out
  - gt_.Tensor #structured
  - ge.Scalar #structured
  - ge.Scalar_out #structured
  - ge_.Scalar #structured
  - ge.Tensor
  - ge.Tensor_out
  - ge_.Tensor #structured
  - cat  # NEED ADD
  - cat.out #NEED ADD
  - isnan
  - isnan.out # autogen
  - masked_fill_.Tensor
  - masked_fill_.Scalar
  - index_add.out
  - index_add_
  - index_add
  - index_select
  - index_select.out
  - gcd
<<<<<<< HEAD
  - gcd.out #structured
  - gcd_ #structured
=======
  - gcd.out
  - gcd_
  - hypot
  - hypot.out
  - hypot_
>>>>>>> main
  - relu
  - relu_
  - relu.out
  - threshold #structured
  - threshold_ #structured
  - threshold.out
  - threshold_backward #structued
  - threshold_backward.grad_input
<<<<<<< HEAD
  - gelu #structured
  - gelu_ #structured
=======
  - softplus
  - softplus.out
  - softplus_backward
  - softplus_backward.grad_input
  - softshrink
  - softshrink.out
  - softshrink_backward
  - softshrink_backward.grad_input
  - mish
  - mish.out
  - mish_
  - mish_backward
  - gelu
  - gelu_
>>>>>>> main
  - gelu.out
  - gelu_backward
<<<<<<< HEAD
  - gelu_backward.grad_input #structured
=======
  - gelu_backward.grad_input
  - leaky_relu
  - leaky_relu_
  - leaky_relu.out
  - leaky_relu_backward
  - leaky_relu_backward.grad_input
>>>>>>> main
  - silu
  - silu_
  - silu.out
  - silu_backward
  - silu_backward.grad_input
  - glu
  - glu.out
  - glu_backward.grad_input
  - glu_backward
  - hardtanh
  - hardtanh.out
  - hardtanh_
  - hardtanh_backward.grad_input
  - hardtanh_backward
  - hardswish
  - hardswish.out
  - hardswish_
  - hardswish_backward
  - arange.start_out
  - abs #structured
  - abs_ #structured
  - abs.out
<<<<<<< HEAD
  - sin #structured
  - sin_ #structured
  - sin.out 
  - cos #structured
  - cos_ #structured
=======
  - sin
  - sin_
  - sin.out
  - sinh
  - sinh_
  - sinh.out
  - cos
  - cos_
>>>>>>> main
  - cos.out
  - log #structured
  - log_ #structured
  - log.out
  - logical_not
  - logical_not_
  - logical_not.out
  - sqrt #structured
  - sqrt_ #structured
  - sqrt
  - sqrt_
  - sqrt.out
  - rsqrt  #structured
  - rsqrt_ #structured
  - rsqrt.out
<<<<<<< HEAD
  - tanh #structured
  - tanh_ #structured
=======
  - tan
  - tan_
  - tan.out
  - tanh
  - tanh_
>>>>>>> main
  - tanh.out
  - neg #structured
  - neg_ #structured
  - neg.out
  - reciprocal #structured
  - reciprocal_ #structured
  - reciprocal.out
  - pow.Tensor_Tensor #structured
  - pow_.Tensor #structured
  - pow.Tensor_Tensor_out
  - pow.Tensor_Scalar #structured
  - pow_.Scalar #structured
  - pow.Tensor_Scalar_out
  - pow.Scalar #structured
  - pow.Scalar_out
  - exp
  - exp.out
  - exp_
  - empty.memory_format
  - empty_strided
<<<<<<< HEAD
<<<<<<< HEAD
  - clone # not sure
=======
=======
  - eye.out
  - eye.m_out
>>>>>>> main
  - _efficientzerotensor
  - complex.out
  - clone
>>>>>>> main
  - fill_.Scalar
  - fill_.Tensor
  - zero_
  - random_
  - random_.from
  - random_.to
  - normal_
  - normal.Tensor_float_out
  - normal.Tensor_float
  - normal.float_Tensor_out
  - normal.float_Tensor
  - normal.Tensor_Tensor_out
  - normal.Tensor_Tensor
  - uniform_
  - bernoulli.out
  - bernoulli_.Tensor
  - bernoulli_.float
  - native_dropout
  - native_dropout_backward # not sure
  - copy_ # not sure
  - _to_copy # not sure
  - view
  - view_as_real
  - view_as_complex
  - as_strided
  - _reshape_alias
  - resize_
  - set_.source_Storage
  - set_.source_Storage_storage_offset
  - set_.source_Tensor
  - set_
  - unfold
  - unfold_backward
  # - resize_as_  # hand-written registration
  # - _copy_from_and_resize  # hand-written registration
  # - _copy_from  # hand-written registration
  - bitwise_and.Tensor_out
  - bitwise_or.Tensor_out
  - bitwise_xor.Tensor_out
  - bitwise_not.out
  - where.self_out
  - where.self
  - clamp
  - clamp.out
  - clamp_
  - clamp.Tensor
  - clamp.Tensor_out
  - clamp_.Tensor
  - clamp_max
  - clamp_max.out
  - clamp_max_
  - clamp_max.Tensor
  - clamp_max.Tensor_out
  - clamp_max_.Tensor
  - clamp_min
  - clamp_min.out
  - clamp_min_
  - clamp_min.Tensor
  - clamp_min.Tensor_out
  - clamp_min_.Tensor
  - native_layer_norm
  - native_layer_norm_backward
  - index.Tensor
  - index.Tensor_out
  - max
  - max.unary_out
  - max.dim_max
  - min
  - min.unary_out
  - min.dim_min
  - sum.out # not sure
  - sum.dim_IntList #not sure
  - sum.IntList_out
  - mean.out
  - mean.dim
  - std.correction
  - std.correction_out
  - std_mean.correction
  - var.correction
  - var.correction_out
  - var_mean.correction
  - all.dim
  - all.out
  - all.dims
  - all.dims_out
  - all
  - all.all_out
  - any.dim # structure
  - any.out
  - any.dims
  - any.dims_out
  - any
  - any.all_out
  - amax
  - amax.out
  - amin
  - amin.out
  - argmax
  - argmax.out
  - _local_scalar_dense
  - max_pool2d_with_indices
  - max_pool2d_with_indices.out
  - max_pool2d_with_indices_backward
  - max_pool2d_with_indices_backward.grad_input
  - embedding_dense_backward
  - _softmax.out
  - _softmax
  - _softmax_backward_data.out
  - _softmax_backward_data
  - _index_put_impl_
  - nonzero
  - nonzero.out
  - _softmax
  - _softmax.out
  - _log_softmax
  - _log_softmax.out
  - _softmax_backward_data
  - _softmax_backward_data.out
  - _log_softmax_backward_data
  - _log_softmax_backward_data.out
  - scatter.src
  - scatter.src_out
  - scatter_.src
  - scatter.value
  - scatter.value_out
  - scatter_.value
  - scatter.reduce
  - scatter.reduce_out
  - scatter_.reduce
  - scatter.value_reduce
  - scatter.value_reduce_out
  - scatter_.value_reduce
  - scatter_add
  - scatter_add.out
  - scatter_add_
  - scatter_reduce.two
  - scatter_reduce.two_out
  - scatter_reduce_.two
  - gather
  - gather.out
  - sort.stable
  - sort.values_stable
  - argsort.stable
  - _foreach_add.Scalar
  - _foreach_add_.Scalar
  - _foreach_add.ScalarList
  - _foreach_add_.ScalarList
  - _foreach_add.List
  - _foreach_add_.List
  - _foreach_mul.Scalar
  - _foreach_mul_.Scalar
  - _foreach_mul.ScalarList
  - _foreach_mul_.ScalarList
  - _foreach_mul.List
  - _foreach_mul_.List
  - _foreach_div.Scalar
  - _foreach_div_.Scalar
  - _foreach_div.ScalarList
  - _foreach_div_.ScalarList
  - _foreach_div.List
  - _foreach_div_.List
  - _foreach_addcmul.Scalar
  - _foreach_addcmul_.Scalar
  - _foreach_addcmul.ScalarList
  - _foreach_addcmul_.ScalarList
  - _foreach_addcmul.Tensor
  - _foreach_addcmul_.Tensor
  - _foreach_addcdiv.Scalar
  - _foreach_addcdiv_.Scalar
  - _foreach_addcdiv.ScalarList
  - _foreach_addcdiv_.ScalarList
  - _foreach_addcdiv.Tensor
  - _foreach_addcdiv_.Tensor
  - _foreach_sqrt
  - _foreach_sqrt_
  - _foreach_lerp.List
  - _foreach_lerp_.List
  - _foreach_lerp.Scalar
  - _foreach_lerp_.Scalar
  - _foreach_norm.Scalar
  - maximum
  - maximum.out
  - minimum
  - minimum.out
  - flip
  - roll
  - sigmoid
  - sigmoid.out
  - sigmoid_
  - sigmoid_backward.grad_input
  - sigmoid_backward
  - hardsigmoid.out
  - hardsigmoid
  - hardsigmoid_
  - hardsigmoid_backward.grad_input
  - hardsigmoid_backward
  - tril.out
  - tril
  - tril_
  - triu.out
  - triu
  - triu_
  - im2col
  - im2col.out
  - col2im
  - col2im.out
  - mse_loss
  - mse_loss.out
  - mse_loss_backward
  - mse_loss_backward.grad_input
  - nll_loss_forward.output
  - nll_loss_forward
  - nll_loss_backward.grad_input
  - nll_loss_backward
  - huber_loss
  - huber_loss.out
  - huber_loss_backward.out
  - batch_norm_stats
  - batch_norm_elemt
  - batch_norm_elemt.out
  - batch_norm_backward_reduce
  - batch_norm_backward_elemt
  - batch_norm_update_stats
  - native_batch_norm
  - native_batch_norm.out
  - native_batch_norm_backward
  - _native_batch_norm_legit
  - _native_batch_norm_legit.out
  - _native_batch_norm_legit.no_stats
  - _native_batch_norm_legit.no_stats_out
  - _batch_norm_with_update
  - _batch_norm_with_update.out
  - batch_norm_backward
  - upsample_bilinear2d
  - upsample_bilinear2d.out
  - upsample_bilinear2d_backward
  - upsample_bilinear2d_backward.grad_input
  - _upsample_nearest_exact1d
  - _upsample_nearest_exact1d.out
  - upsample_nearest1d
  - upsample_nearest1d.out
  - _upsample_nearest_exact1d_backward
  - _upsample_nearest_exact1d_backward.grad_input
  - upsample_nearest1d_backward
  - upsample_nearest1d_backward.grad_input
  - upsample_nearest2d
  - upsample_nearest2d.out
  - upsample_nearest2d_backward
  - upsample_nearest2d_backward.grad_input
  - _upsample_nearest_exact2d
  - _upsample_nearest_exact2d.out
  - _upsample_nearest_exact2d_backward
  - _upsample_nearest_exact2d_backward.grad_input
  - upsample_bicubic2d
  - upsample_bicubic2d.out
  - bincount
  - _embedding_bag
  - _embedding_bag_forward_only
  - sgn
  - sgn.out
  - sgn_
  - _cdist_forward
  - _pin_memory
  - is_pinned
  - is_set_to
  - bucketize.Tensor
  - bucketize.Tensor_out
  - bucketize.Scalar
  - searchsorted.Tensor
  - searchsorted.Tensor_out
  - searchsorted.Scalar
  - searchsorted.Scalar_out
  - trace
  - reflection_pad1d
  - reflection_pad1d.out
  - reflection_pad1d_backward
  - reflection_pad1d_backward.grad_input
  - reflection_pad2d
  - reflection_pad2d.out
  - reflection_pad2d_backward
  - reflection_pad2d_backward.grad_input
  - reflection_pad3d
  - reflection_pad3d.out
  - reflection_pad3d_backward
  - reflection_pad3d_backward.grad_input
  - native_group_norm
  - native_group_norm_backward
  - elu
  - elu.out
  - elu_
  - elu_backward
  - elu_backward.grad_input
  - erfc
  - erfc_
  - erfc.out
  - multinomial
  - multinomial.out
  - erf
  - erf_
  - erf.out
  - linalg_vector_norm
  - linalg_vector_norm.out
  - grid_sampler_2d
  - grid_sampler_2d_backward
  - acos
  - acos_
  - acos.out
  - acosh
  - acosh_
  - acosh.out
  - addr
  - addr.out
  - avg_pool2d
  - avg_pool2d.out
  - avg_pool2d_backward
  - avg_pool2d_backward.grad_input
  - addcdiv.out
  - addcdiv
  - addcdiv_
  - addcmul.out
  - addcmul
  - addcmul_
  - asinh
  - asinh.out
  - asinh_
  - asin
  - asin.out
  - asin_
  - atan
  - atan.out
  - atan_
  - atan2
  - atan2.out
  - atan2_
  - atanh
  - atanh.out
  - atanh_
  - cosh
  - cosh.out
  - cosh_
  - randperm.generator_out
  - _amp_foreach_non_finite_check_and_unscale_
  - _amp_update_scale_
  - linalg_cross
  - linalg_cross.out
  - copysign.out
  - copysign.Tensor
  - copysign_.Tensor
  - count_nonzero.dim_IntList
  - conj_physical.out
  - conj_physical_
  - ceil
  - ceil_
  - ceil.out
  - nan_to_num.out
