- func: copy(Tensor self, Tensor src, bool non_blocking=False) -> Tensor
  variants: function
  tags: core

- func: copy_(Tensor(a!) self, Tensor src, bool non_blocking=False) -> Tensor(a!)
  variants: method
  device_check: NoCheck
  device_guard: False
  autogen: copy.out

- func: _copy_from(Tensor self, Tensor dst, bool non_blocking=False) -> Tensor
  autogen: _copy_from.out

# We need this to be able to properly copy from a CPU to an XLA tensor with different sizes.
# See https://github.com/pytorch/xla/issues/2881
- func: _copy_from_and_resize(Tensor self, Tensor dst) -> Tensor
  dispatch:
    MPS: _copy_from_and_resize_mps
  autogen: _copy_from_and_resize.out

- func: add.Tensor(Tensor self, Tensor other, *, Scalar alpha=1) -> Tensor
  device_check: NoCheck   # TensorIterator
  structured_delegate: add.out
  variants: function, method
  tags: [core, pointwise]

- func: _to_copy(Tensor self, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, bool non_blocking=False, MemoryFormat? memory_format=None) -> Tensor
  device_check: NoCheck
  device_guard: False
  autogen: _to_copy.out
  tags: core

# to(Device) must not exist because all constructors of Device also works for
# TensorOptions. Otherwise, an ambiguity error is thrown.
# See NOTE [ TensorOptions Constructors ].
- func: to.dtype_layout(Tensor(a) self, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, bool non_blocking=False, bool copy=False, MemoryFormat? memory_format=None) -> Tensor(a)
  variants: method
  device_check: NoCheck
  device_guard: False

- func: to.device(Tensor(a) self, Device device, ScalarType dtype, bool non_blocking=False, bool copy=False, MemoryFormat? memory_format=None) -> Tensor(a)
  variants: method
  device_check: NoCheck
  device_guard: False

- func: to.dtype(Tensor(a) self, ScalarType dtype, bool non_blocking=False, bool copy=False, MemoryFormat? memory_format=None) -> Tensor(a)
  variants: method
  device_check: NoCheck
  device_guard: False

- func: to.other(Tensor(a) self, Tensor other, bool non_blocking=False, bool copy=False, MemoryFormat? memory_format=None) -> Tensor(a)
  variants: method
  device_check: NoCheck
  device_guard: False

- func: add_.Tensor(Tensor(a!) self, Tensor other, *, Scalar alpha=1) -> Tensor(a!)
  device_check: NoCheck   # TensorIterator
  variants: method
  structured_delegate: add.out
  tags: pointwise

- func: add.out(Tensor self, Tensor other, *, Scalar alpha=1, Tensor(a!) out) -> Tensor(a!)
  device_check: NoCheck   # TensorIterator
  structured: True
  structured_inherits: TensorIteratorBase
  ufunc_inner_loop:
    Generic: add (AllAndComplex, BFloat16, Half, ComplexHalf)
    ScalarOnly: add (Bool)
  tags: pointwise

# - func: cumsum(Tensor self, int dim, *, ScalarType? dtype=None) -> Tensor
#   structured_delegate: cumsum.out
#   device_check: NoCheck   # TensorIterator
#   variants: function, method
#   tags: core

# - func: cumsum_(Tensor(a!) self, int dim, *, ScalarType? dtype=None) -> Tensor(a!)
#   structured_delegate: cumsum.out
#   variants: method

# - func: cumsum.out(Tensor self, int dim, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)
#   structured: True
#   device_check: NoCheck   # TensorIterator
#   dispatch:
#     XPU : cumsum_out

- func: sub.out(Tensor self, Tensor other, *, Scalar alpha=1, Tensor(a!) out) -> Tensor(a!)
  device_check: NoCheck   # TensorIterator
  structured: True
  structured_inherits: TensorIteratorBase
  dispatch:
    XPU: sub_out
  tags: pointwise

- func: sub.Tensor(Tensor self, Tensor other, *, Scalar alpha=1) -> Tensor
  device_check: NoCheck   # TensorIterator
  variants: function, method
  structured_delegate: sub.out
  tags: [core, pointwise]

- func: sub_.Tensor(Tensor(a!) self, Tensor other, *, Scalar alpha=1) -> Tensor(a!)
  device_check: NoCheck   # TensorIterator
  variants: method
  structured_delegate: sub.out
  tags: pointwise
# For C++ only, until we have conversion from C++ numbers to Tensor

- func: sub.Scalar(Tensor self, Scalar other, Scalar alpha=1) -> Tensor
  device_check: NoCheck   # TensorIterator
  variants: function, method
  tags: [core, pointwise]

- func: sub_.Scalar(Tensor(a!) self, Scalar other, Scalar alpha=1) -> Tensor(a!)
  device_check: NoCheck   # TensorIterator
  variants: method
  autogen: sub.Scalar_out
  tags: pointwise
# subtract, alias for sub

- func: mul.Tensor(Tensor self, Tensor other) -> Tensor
  device_check: NoCheck   # TensorIterator
  structured_delegate: mul.out
  variants: function, method
  tags: [core, pointwise]

- func: mul_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)
  device_check: NoCheck   # TensorIterator
  structured_delegate: mul.out
  variants: method
  tags: pointwise

- func: mul.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
  device_check: NoCheck   # TensorIterator
  structured: True
  structured_inherits: TensorIteratorBase
  dispatch:
    XPU: mul_out
  tags: pointwise
  # For C++ only, until we have conversion from C++ numbers to Tensor

- func: mul.Scalar(Tensor self, Scalar other) -> Tensor
  device_check: NoCheck   # TensorIterator
  variants: function, method
  tags: [core, pointwise]

- func: mul_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)
  device_check: NoCheck   # TensorIterator
  variants: method
  autogen: mul.Scalar_out
  tags: pointwise
# multiply, alias for mul


# - func: div.Tensor(Tensor self, Tensor other) -> Tensor
#   device_check: NoCheck   # TensorIterator
#   variants: function, method
#   structured_delegate: div.out
#   tags: [core, pointwise]

# - func: div_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)
#   device_check: NoCheck   # TensorIterator
#   variants: method
#   structured_delegate: div.out
#   tags: pointwise

# - func: div.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
#   device_check: NoCheck   # TensorIterator
#   structured: True
#   structured_inherits: TensorIteratorBase
#   dispatch:
#     XPU: div_out
#   tags: pointwise

# - func: rsub.Tensor(Tensor self, Tensor other, *, Scalar alpha=1) -> Tensor
#   device_check: NoCheck   # TensorIterator
#   variants: function
#   dispatch:
#     XPU: rsub
#   autogen: rsub.Tensor_out

- func: remainder.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
  device_check: NoCheck   # TensorIterator
  structured: True
  structured_inherits: TensorIteratorBase
  dispatch:
    XPU: remainder_out
  tags: pointwise

- func: remainder.Tensor(Tensor self, Tensor other) -> Tensor
  device_check: NoCheck   # TensorIterator
  structured_delegate: remainder.Tensor_out
  variants: method, function
  tags: [core, pointwise]

- func: remainder_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)
  device_check: NoCheck   # TensorIterator
  structured_delegate: remainder.Tensor_out
  variants: method
  tags: pointwise

- func: fmod.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
  device_check: NoCheck   # TensorIterator
  structured: True
  structured_inherits: TensorIteratorBase
  dispatch:
    XPU: fmod_out
  tags: pointwise

- func: fmod.Tensor(Tensor self, Tensor other) -> Tensor
  device_check: NoCheck   # TensorIterator
  structured_delegate: fmod.Tensor_out
  variants: method, function
  tags: [core, pointwise]

- func: fmod_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)
  device_check: NoCheck   # TensorIterator
  variants: method
  structured_delegate: fmod.Tensor_out
  tags: pointwise

- func: tanh_backward.grad_input(Tensor grad_output, Tensor output, *, Tensor(a!) grad_input) -> Tensor(a!)
  python_module: nn
  structured: True
  structured_inherits: TensorIteratorBase
  dispatch:
    XPU: tanh_backward_out
  tags: pointwise

- func: tanh_backward(Tensor grad_output, Tensor output) -> Tensor
  python_module: nn
  structured_delegate: tanh_backward.grad_input

- func: eq_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)
  structured_delegate: eq.Scalar_out
  device_check: NoCheck   # TensorIterator
  variants: method

- func: eq.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
  structured: True
  structured_inherits: TensorIteratorBase
  device_check: NoCheck   # TensorIterator
  dispatch:
    XPU: eq_Scalar_out
  tags: pointwise

- func: eq.Scalar(Tensor self, Scalar other) -> Tensor
  structured_delegate: eq.Scalar_out
  device_check: NoCheck   # TensorIterator
  variants: method, function
  tags: [core, pointwise]

- func: eq.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
  structured: True
  structured_inherits: TensorIteratorBase
  device_check: NoCheck   # TensorIterator
  dispatch:
    XPU: eq_Tensor_out
  tags: pointwise

- func: eq.Tensor(Tensor self, Tensor other) -> Tensor
  structured_delegate: eq.Tensor_out
  device_check: NoCheck   # TensorIterator
  variants: method, function
  tags: [core, pointwise]

- func: eq_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)
  structured_delegate: eq.Tensor_out
  device_check: NoCheck   # TensorIterator
  variants: method

- func: ne.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
  structured: True
  structured_inherits: TensorIteratorBase
  device_check: NoCheck   # TensorIterator
  dispatch:
    XPU: ne_Scalar_out
  tags: pointwise

- func: ne.Scalar(Tensor self, Scalar other) -> Tensor
  structured_delegate: ne.Scalar_out
  device_check: NoCheck   # TensorIterator
  variants: method, function
  tags: [core, pointwise]

- func: ne.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
  structured: True
  structured_inherits: TensorIteratorBase
  device_check: NoCheck   # TensorIterator
  dispatch:
    XPU: ne_Tensor_out
  tags: pointwise

- func: ne.Tensor(Tensor self, Tensor other) -> Tensor
  structured_delegate: ne.Tensor_out
  device_check: NoCheck   # TensorIterator
  variants: method, function
  tags: [core, pointwise]

- func: ne_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)
  structured_delegate: ne.Scalar_out
  device_check: NoCheck   # TensorIterator
  variants: method

- func: ne_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)
  structured_delegate: ne.Tensor_out
  device_check: NoCheck   # TensorIterator
  variants: method

- func: lt.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
  structured: True
  structured_inherits: TensorIteratorBase
  device_check: NoCheck   # TensorIterator
  dispatch:
    XPU: lt_Scalar_out
  tags: pointwise

- func: lt.Scalar(Tensor self, Scalar other) -> Tensor
  structured_delegate: lt.Scalar_out
  device_check: NoCheck   # TensorIterator
  variants: method, function
  tags: [core, pointwise]

- func: lt.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
  structured: True
  structured_inherits: TensorIteratorBase
  device_check: NoCheck   # TensorIterator
  dispatch:
    XPU: lt_Tensor_out
  tags: pointwise

- func: lt.Tensor(Tensor self, Tensor other) -> Tensor
  structured_delegate: lt.Tensor_out
  device_check: NoCheck   # TensorIterator
  variants: method, function
  tags: [core, pointwise]

- func: lt_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)
  structured_delegate: lt.Scalar_out
  device_check: NoCheck   # TensorIterator
  variants: method

- func: lt_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)
  structured_delegate: lt.Tensor_out
  device_check: NoCheck   # TensorIterator
  variants: method

- func: le.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
  structured: True
  structured_inherits: TensorIteratorBase
  device_check: NoCheck   # TensorIterator
  dispatch:
    XPU: le_Scalar_out
  tags: pointwise

- func: le.Scalar(Tensor self, Scalar other) -> Tensor
  structured_delegate: le.Scalar_out
  device_check: NoCheck   # TensorIterator
  variants: method, function
  tags: [core, pointwise]

- func: le.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
  structured: True
  structured_inherits: TensorIteratorBase
  device_check: NoCheck   # TensorIterator
  dispatch:
    XPU: le_Tensor_out
  tags: pointwise

- func: le.Tensor(Tensor self, Tensor other) -> Tensor
  structured_delegate: le.Tensor_out
  device_check: NoCheck   # TensorIterator
  variants: method, function
  tags: [core, pointwise]

- func: le_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)
  structured_delegate: le.Scalar_out
  device_check: NoCheck   # TensorIterator
  variants: method

- func: le_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)
  structured_delegate: le.Tensor_out
  device_check: NoCheck   # TensorIterator
  variants: method

- func: gt.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
  structured: True
  structured_inherits: TensorIteratorBase
  device_check: NoCheck   # TensorIterator
  dispatch:
    XPU: gt_Scalar_out
  tags: pointwise

- func: gt.Scalar(Tensor self, Scalar other) -> Tensor
  structured_delegate: gt.Scalar_out
  device_check: NoCheck   # TensorIterator
  variants: method, function
  tags: [core, pointwise]

- func: gt.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
  structured: True
  structured_inherits: TensorIteratorBase
  device_check: NoCheck   # TensorIterator
  dispatch:
    XPU: gt_Tensor_out
  tags: pointwise

- func: gt.Tensor(Tensor self, Tensor other) -> Tensor
  structured_delegate: gt.Tensor_out
  device_check: NoCheck   # TensorIterator
  variants: method, function
  tags: [core, pointwise]

- func: gt_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)
  structured_delegate: gt.Scalar_out
  device_check: NoCheck   # TensorIterator
  variants: method

- func: gt_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)
  structured_delegate: gt.Tensor_out
  device_check: NoCheck   # TensorIterator
  variants: method

- func: ge.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
  structured: True
  structured_inherits: TensorIteratorBase
  device_check: NoCheck   # TensorIterator
  dispatch:
    XPU: ge_Scalar_out
  tags: pointwise

- func: ge.Scalar(Tensor self, Scalar other) -> Tensor
  structured_delegate: ge.Scalar_out
  device_check: NoCheck   # TensorIterator
  variants: method, function
  tags: [core, pointwise]

- func: ge.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
  structured: True
  structured_inherits: TensorIteratorBase
  device_check: NoCheck   # TensorIterator
  dispatch:
    XPU: ge_Tensor_out
  tags: pointwise

- func: ge.Tensor(Tensor self, Tensor other) -> Tensor
  structured_delegate: ge.Tensor_out
  device_check: NoCheck   # TensorIterator
  variants: method, function
  tags: [core, pointwise]

- func: ge_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)
  structured_delegate: ge.Scalar_out
  device_check: NoCheck   # TensorIterator
  variants: method

- func: ge_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)
  structured_delegate: ge.Tensor_out
  device_check: NoCheck   # TensorIterator
  variants: method

- func: isnan(Tensor self) -> Tensor
  variants: function, method
  device_check: NoCheck
  device_guard: False
  dispatch:
    XPU: isnan
  autogen: isnan.out
  tags: [core, pointwise]

# - func: masked_fill_.Scalar(Tensor(a!) self, Tensor mask, Scalar value) -> Tensor(a!)
#   device_check: NoCheck   # TensorIterator
#   variants: method
#   dispatch:
#     XPU: masked_fill__xpu
#   autogen: masked_fill.Scalar_out

# - func: masked_fill.Scalar(Tensor self, Tensor mask, Scalar value) -> Tensor
#   device_check: NoCheck   # TensorIterator
#   variants: function, method
#   tags: pointwise

# - func: masked_fill_.Tensor(Tensor(a!) self, Tensor mask, Tensor value) -> Tensor(a!)
#   device_check: NoCheck   # TensorIterator
#   variants: method
#   dispatch:
#     XPU: masked_fill__xpu
#   autogen: masked_fill.Tensor_out

# - func: masked_fill.Tensor(Tensor self, Tensor mask, Tensor value) -> Tensor
#   device_check: NoCheck   # TensorIterator
#   variants: function, method

- func: index_add.out(Tensor self, int dim, Tensor index, Tensor source, *, Scalar alpha=1, Tensor(a!) out) -> Tensor(a!)
  structured: True
  variants: function
  precomputed:
  - dim -> int dim
  dispatch:
    XPU: index_add_xpu_out

- func: index_add_(Tensor(a!) self, int dim, Tensor index, Tensor source, *, Scalar alpha=1) -> Tensor(a!)
  structured_delegate: index_add.out
  variants: method

- func: index_add(Tensor self, int dim, Tensor index, Tensor source, *, Scalar alpha=1) -> Tensor
  structured_delegate: index_add.out
  variants: function, method

- func: index_select.out(Tensor self, int dim, Tensor index, *, Tensor(a!) out) -> Tensor(a!)
  dispatch:
    XPU: index_select_out_xpu

- func: index_select(Tensor self, int dim, Tensor index) -> Tensor
  variants: method, function
  dispatch:
    XPU: index_select_xpu_
  tags: core

- func: gcd.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
  structured: True
  structured_inherits: TensorIteratorBase
  dispatch:
    XPU: gcd_out
  tags: pointwise

- func: gcd(Tensor self, Tensor other) -> Tensor
  structured_delegate: gcd.out
  variants: function, method
  tags: pointwise

- func: gcd_(Tensor(a!) self, Tensor other) -> Tensor(a!)
  structured_delegate: gcd.out
  variants: function, method

# - func: relu(Tensor self) -> Tensor
#   device_check: NoCheck   # TensorIterator
#   variants: function, method
#   dispatch:
#     XPU: relu
#   tags: [core, pointwise]

# - func: relu_(Tensor(a!) self) -> Tensor(a!)
#   device_check: NoCheck   # TensorIterator
#   variants: function, method
#   dispatch:
#     XPU: relu_
#   autogen: relu.out
#   tags: pointwise

- func: threshold(Tensor self, Scalar threshold, Scalar value) -> Tensor
  device_check: NoCheck   # TensorIterator
  variants: function
  structured_delegate: threshold.out
  dispatch:
    QuantizedCPU: threshold_quantized_cpu

- func: threshold_(Tensor(a!) self, Scalar threshold, Scalar value) -> Tensor(a!)
  device_check: NoCheck   # TensorIterator
  variants: function
  structured_delegate: threshold.out

- func: threshold.out(Tensor self, Scalar threshold, Scalar value, *, Tensor(a!) out) -> Tensor(a!)
  device_check: NoCheck   # TensorIterator
  structured: True
  structured_inherits: TensorIteratorBase
  dispatch:
    XPU: threshold_out

- func: threshold_backward.grad_input(Tensor grad_output, Tensor self, Scalar threshold, *, Tensor(a!) grad_input) -> Tensor(a!)
  structured: True
  structured_inherits: TensorIteratorBase
  dispatch:
    XPU: threshold_backward_out

- func: threshold_backward(Tensor grad_output, Tensor self, Scalar threshold) -> Tensor
  variants: function
  structured_delegate: threshold_backward.grad_input
  tags: pointwise

- func: gelu.out(Tensor self, *, str approximate='none', Tensor(a!) out) -> Tensor(a!)
  structured: True
  structured_inherits: TensorIteratorBase
  device_check: NoCheck   # TensorIterator
  python_module: nn
  dispatch:
    XPU: gelu_out_xpu

- func: gelu_(Tensor(a!) self, *, str approximate='none') -> Tensor(a!)
  structured_delegate: gelu.out
  device_check: NoCheck   # TensorIterator
  python_module: nn

- func: gelu(Tensor self, *, str approximate='none') -> Tensor
  structured_delegate: gelu.out
  device_check: NoCheck   # TensorIterator
  python_module: nn
  tags: [core, pointwise]

- func: gelu_backward.grad_input(Tensor grad_output, Tensor self, *, str approximate='none', Tensor(a!) grad_input) -> Tensor(a!)
  structured: True
  structured_inherits: TensorIteratorBase
  python_module: nn
  dispatch:
    XPU: gelu_backward_out_xpu

- func: gelu_backward(Tensor grad_output, Tensor self, *, str approximate='none') -> Tensor
  structured_delegate: gelu_backward.grad_input
  python_module: nn
  tags: pointwise

- func: arange(Scalar end, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
  dispatch:
    CompositeExplicitAutograd: arange

- func: arange.start(Scalar start, Scalar end, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
  dispatch:
    CompositeExplicitAutograd: arange

- func: arange.start_step(Scalar start, Scalar end, Scalar step=1, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
  dispatch:
    CompositeExplicitAutograd: arange
  cpp_no_default_args: ['step']
  tags: core

- func: arange.start_out(Scalar start, Scalar end, Scalar step=1, *, Tensor(a!) out) -> Tensor(a!)
  dispatch:
    XPU: arange_out
  cpp_no_default_args: ['step']

- func: abs.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
  device_check: NoCheck   # TensorIterator
  dispatch:
    XPU: abs_out
  tags: pointwise

- func: abs(Tensor self) -> Tensor
  device_check: NoCheck   # TensorIterator
  variants: function, method
  tags: [core, pointwise]

- func: abs_(Tensor(a!) self) -> Tensor(a!)
  device_check: NoCheck   # TensorIterator
  variants: function, method

- func: sin(Tensor self) -> Tensor
  device_check: NoCheck   # TensorIterator
  structured_delegate: sin.out
  variants: function, method
  tags: [core, pointwise]

- func: sin_(Tensor(a!) self) -> Tensor(a!)
  device_check: NoCheck   # TensorIterator
  structured_delegate: sin.out
  variants: function, method
  tags: pointwise

- func: sin.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
  device_check: NoCheck   # TensorIterator
  structured: True
  structured_inherits: TensorIteratorBase
  dispatch:
    XPU: sin_out
  tags: pointwise

- func: cos(Tensor self) -> Tensor
  device_check: NoCheck   # TensorIterator
  variants: function, method
  structured_delegate: cos.out
  dispatch:
    NestedTensorCPU, NestedTensorCUDA: cos_nested
  tags: [core, pointwise]

- func: cos_(Tensor(a!) self) -> Tensor(a!)
  device_check: NoCheck   # TensorIterator
  variants: function, method
  structured_delegate: cos.out
  tags: pointwise

- func: cos.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
  device_check: NoCheck   # TensorIterator
  structured: True
  structured_inherits: TensorIteratorBase
  dispatch:
    XPU: cos_out
  tags: pointwise

- func: log(Tensor self) -> Tensor
  device_check: NoCheck   # TensorIterator
  structured_delegate: log.out
  variants: function, method
  tags: [core, pointwise]

- func: log_(Tensor(a!) self) -> Tensor(a!)
  device_check: NoCheck   # TensorIterator
  structured_delegate: log.out
  variants: function, method
  tags: pointwise

- func: log.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
  device_check: NoCheck   # TensorIterator
  structured: True
  structured_inherits: TensorIteratorBase
  dispatch:
    XPU: log_out
  tags: pointwise

- func: sqrt(Tensor self) -> Tensor
  device_check: NoCheck   # TensorIterator
  structured_delegate: sqrt.out
  variants: function, method
  tags: [core, pointwise]

- func: sqrt_(Tensor(a!) self) -> Tensor(a!)
  device_check: NoCheck   # TensorIterator
  structured_delegate: sqrt.out
  variants: function, method
  tags: pointwise

- func: sqrt.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
  device_check: NoCheck   # TensorIterator
  structured: True
  structured_inherits: TensorIteratorBase
  dispatch:
    XPU: sqrt_out
  tags: pointwise

- func: rsqrt(Tensor self) -> Tensor
  device_check: NoCheck   # TensorIterator
  structured_delegate: rsqrt.out
  variants: function, method
  tags: [core, pointwise]

- func: rsqrt_(Tensor(a!) self) -> Tensor(a!)
  device_check: NoCheck   # TensorIterator
  structured_delegate: rsqrt.out
  variants: function, method
  tags: pointwise

- func: rsqrt.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
  device_check: NoCheck   # TensorIterator
  structured: True
  structured_inherits: TensorIteratorBase
  dispatch:
    XPU: rsqrt_out
  tags: pointwise

- func: tanh(Tensor self) -> Tensor
  device_check: NoCheck   # TensorIterator
  structured_delegate: tanh.out
  variants: function, method
  tags: [core, pointwise]

- func: tanh_(Tensor(a!) self) -> Tensor(a!)
  device_check: NoCheck   # TensorIterator
  structured_delegate: tanh.out
  variants: function, method
  tags: pointwise

- func: tanh.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
  device_check: NoCheck   # TensorIterator
  structured: True
  structured_inherits: TensorIteratorBase
  dispatch:
    XPU: tanh_out
  tags: pointwise

- func: neg(Tensor self) -> Tensor
  device_check: NoCheck   # TensorIterator
  structured_delegate: neg.out
  variants: function, method
  tags: [core, pointwise]

- func: neg_(Tensor(a!) self) -> Tensor(a!)
  device_check: NoCheck   # TensorIterator
  structured_delegate: neg.out
  variants: function, method
  tags: pointwise

- func: neg.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
  device_check: NoCheck   # TensorIterator
  structured: True
  structured_inherits: TensorIteratorBase
  dispatch:
    XPU: neg_out
  tags: pointwise

- func: reciprocal(Tensor self) -> Tensor
  device_check: NoCheck   # TensorIterator
  structured_delegate: reciprocal.out
  variants: function, method
  tags: [core, pointwise]

- func: reciprocal_(Tensor(a!) self) -> Tensor(a!)
  device_check: NoCheck   # TensorIterator
  structured_delegate: reciprocal.out
  variants: function, method
  tags: pointwise

- func: reciprocal.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
  device_check: NoCheck   # TensorIterator
  structured: True
  structured_inherits: TensorIteratorBase
  dispatch:
    XPU: reciprocal_out
  tags: pointwise

- func: pow.Tensor_Tensor_out(Tensor self, Tensor exponent, *, Tensor(a!) out) -> Tensor(a!)
  device_check: NoCheck   # TensorIterator
  structured: True
  structured_inherits: TensorIteratorBase
  dispatch:
    XPU: pow_Tensor_Tensor_out
  tags: pointwise

- func: pow.Tensor_Tensor(Tensor self, Tensor exponent) -> Tensor
  device_check: NoCheck   # TensorIterator
  structured_delegate: pow.Tensor_Tensor_out
  variants: method, function
  tags: [core, pointwise]

- func: pow.Scalar_out(Scalar self, Tensor exponent, *, Tensor(a!) out) -> Tensor(a!)
  device_check: NoCheck   # TensorIterator
  structured: True
  dispatch:
    XPU: pow_Scalar_out
  tags: pointwise

- func: pow.Scalar(Scalar self, Tensor exponent) -> Tensor
  device_check: NoCheck   # TensorIterator
  structured_delegate: pow.Scalar_out
  tags: [core, pointwise]

- func: pow.Tensor_Scalar_out(Tensor self, Scalar exponent, *, Tensor(a!) out) -> Tensor(a!)
  device_check: NoCheck   # TensorIterator
  structured: True
  structured_inherits: TensorIteratorBase
  dispatch:
    XPU: pow_Tensor_Scalar_out
  tags: pointwise

- func: pow.Tensor_Scalar(Tensor self, Scalar exponent) -> Tensor
  device_check: NoCheck   # TensorIterator
  structured_delegate: pow.Tensor_Scalar_out
  variants: function, method
  dispatch:
    SparseCPU, SparseCUDA: pow_sparse_scalar
  tags: [core, pointwise]

- func: pow_.Scalar(Tensor(a!) self, Scalar exponent) -> Tensor(a!)
  device_check: NoCheck   # TensorIterator
  structured_delegate: pow.Tensor_Scalar_out
  variants: method
  tags: pointwise

- func: pow_.Tensor(Tensor(a!) self, Tensor exponent) -> Tensor(a!)
  device_check: NoCheck   # TensorIterator
  structured_delegate: pow.Tensor_Tensor_out
  variants: method
  tags: pointwise

- func: empty.memory_format(SymInt[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, MemoryFormat? memory_format=None) -> Tensor
  dispatch:
    XPU: empty_xpu
  tags: core

- func: empty_strided(SymInt[] size, SymInt[] stride, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
  dispatch:
    XPU: empty_strided_xpu
  autogen: empty_strided.out
  tags: core

- func: fill.Scalar(Tensor self, Scalar value) -> Tensor
  variants: function
  tags: core

- func: fill.Tensor(Tensor self, Tensor value) -> Tensor
  variants: function

- func: fill_.Scalar(Tensor(a!) self, Scalar value) -> Tensor(a!)
  device_check: NoCheck   # TensorIterator
  variants: function, method
  dispatch:
    XPU: fill_
  autogen: fill.Scalar_out

- func: fill_.Tensor(Tensor(a!) self, Tensor value) -> Tensor(a!)
  device_check: NoCheck   # TensorIterator
  variants: function, method
  dispatch:
    XPU: fill_
  autogen: fill.Tensor_out

# - func: zero_(Tensor(a!) self) -> Tensor(a!)
#   device_check: NoCheck   # TensorIterator
#   variants: method, function
#   dispatch:
#     XPU: zero_
#   autogen: zero, zero.out

- func: random_.from(Tensor(a!) self, int from, int? to, *, Generator? generator=None) -> Tensor(a!)
  device_check: NoCheck   # TensorIterator
  variants: method
  tags: nondeterministic_seeded
  dispatch:
    XPU: random_
  autogen: random.from, random.from_out

- func: random_.to(Tensor(a!) self, int to, *, Generator? generator=None) -> Tensor(a!)
  device_check: NoCheck   # TensorIterator
  tags: nondeterministic_seeded
  variants: method
  dispatch:
    XPU: random_
  autogen: random.to, random.to_out

- func: random_(Tensor(a!) self, *, Generator? generator=None) -> Tensor(a!)
  device_check: NoCheck   # TensorIterator
  tags: nondeterministic_seeded
  variants: method
  dispatch:
    XPU: random_
  autogen: random, random.out

- func: normal_(Tensor(a!) self, float mean=0, float std=1, *, Generator? generator=None) -> Tensor(a!)
  device_check: NoCheck   # TensorIterator
  tags: nondeterministic_seeded
  variants: method
  dispatch:
    XPU: normal_
  autogen: normal.out

# Only used by the functionalization pass.
# Normally, the codegen would be able to generate a normal() NativeFunction,
# but we can't due to overload ambiguity with normal.Tensor_float.
- func: normal_functional(Tensor self, float mean=0, float std=1, *, Generator? generator=None) -> Tensor
  device_check: NoCheck   # TensorIterator
  tags: nondeterministic_seeded

- func: normal.Tensor_float_out(Tensor mean, float std=1, *, Generator? generator=None, Tensor(a!) out) -> Tensor(a!)
  tags: nondeterministic_seeded
  dispatch:
    XPU: normal_out

- func: normal.Tensor_float(Tensor mean, float std=1, *, Generator? generator=None) -> Tensor
  dispatch:
    XPU: normal
  tags: nondeterministic_seeded

- func: normal.float_Tensor_out(float mean, Tensor std, *, Generator? generator=None, Tensor(a!) out) -> Tensor(a!)
  dispatch:
    XPU: normal_out
  tags: nondeterministic_seeded

- func: normal.float_Tensor(float mean, Tensor std, *, Generator? generator=None) -> Tensor
  dispatch:
    XPU: normal
  tags: nondeterministic_seeded

- func: normal.Tensor_Tensor_out(Tensor mean, Tensor std, *, Generator? generator=None, Tensor(a!) out) -> Tensor(a!)
  dispatch:
    XPU: normal_out
  tags: nondeterministic_seeded

- func: normal.Tensor_Tensor(Tensor mean, Tensor std, *, Generator? generator=None) -> Tensor
  dispatch:
    XPU: normal
  tags: nondeterministic_seeded

- func: normal.float_float(float mean, float std, SymInt[] size, *, Generator? generator=None, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
  dispatch:
    CompositeExplicitAutograd: normal
  tags: nondeterministic_seeded

- func: normal.float_float_out(float mean, float std, SymInt[] size, *, Generator? generator=None, Tensor(a!) out) -> Tensor(a!)
  dispatch:
    CompositeExplicitAutograd: normal
  tags: nondeterministic_seeded

- func: uniform_(Tensor(a!) self, float from=0, float to=1, *, Generator? generator=None) -> Tensor(a!)
  device_check: NoCheck   # TensorIterator
  tags: nondeterministic_seeded
  variants: method
  dispatch:
    XPU: uniform_
  autogen: uniform, uniform.out

# Sample bernoulli with values in `self` as probability.
- func: bernoulli(Tensor self, *, Generator? generator=None) -> Tensor
  device_check: NoCheck   # TensorIterator
  variants: function, method
  tags: nondeterministic_seeded

- func: bernoulli.out(Tensor self, *, Generator? generator=None, Tensor(a!) out) -> Tensor(a!)
  device_check: NoCheck   # TensorIterator
  variants: function
  tags: nondeterministic_seeded
  dispatch:
    XPU: bernoulli_out

- func: bernoulli_.Tensor(Tensor(a!) self, Tensor p, *, Generator? generator=None) -> Tensor(a!)
  device_check: NoCheck   # TensorIterator
  variants: method
  tags: nondeterministic_seeded
  dispatch:
    XPU: bernoulli_
  autogen: bernoulli.Tensor, bernoulli.Tensor_out

- func: bernoulli_.float(Tensor(a!) self, float p=0.5, *, Generator? generator=None) -> Tensor(a!)
  device_check: NoCheck   # TensorIterator
  variants: method
  tags: nondeterministic_seeded
  dispatch:
    XPU: bernoulli_
  autogen: bernoulli.float_out

# Note [bernoulli.p schema]
# We should probably just fix the overload ambiguity by appending a _functional to the C++ API name (BC breaking)
# This out-of-place version isn't used explicitly, but needed by jit.
# There is no default valid on `p` here because it would introduce ambiguity
# with `bernoulli(Tensor self, *, Generator? generator=None)` declaration.
- func: bernoulli.p(Tensor self, float p, *, Generator? generator=None) -> Tensor
  device_check: NoCheck   # TensorIterator
  variants: function, method
  tags: nondeterministic_seeded

- func: native_dropout(Tensor input, float p, bool? train) -> (Tensor, Tensor)
  variants: function
  dispatch:
    XPU: native_dropout_xpu
  tags: [nondeterministic_seeded, core]
  autogen: native_dropout.out

- func: native_dropout_backward(Tensor grad_output, Tensor mask, float scale) -> Tensor
  dispatch:
    XPU: native_dropout_backward_xpu
  autogen: native_dropout_backward.out
  tags: pointwise

- func: view(Tensor(a) self, SymInt[] size) -> Tensor(a)
  variants: method
  device_check: NoCheck
  device_guard: False
  dispatch:
    XPU: view
  tags: core

- func: view_as_real(Tensor(a) self) -> Tensor(a)
  variants: function
  dispatch:
    XPU: view_as_real

- func: view_as_complex(Tensor(a) self) -> Tensor(a)
  variants: function
  dispatch:
    XPU: view_as_complex

- func: view_copy(Tensor self, SymInt[] size) -> Tensor
  variants: function
  dispatch:
    CompositeExplicitAutogradNonFunctional: view_copy_symint
  tags: view_copy
  autogen: view_copy.out

- func: view_as_real_copy(Tensor self) -> Tensor
  variants: function
  dispatch:
    CompositeExplicitAutogradNonFunctional: view_as_real_copy
  tags: view_copy
  autogen: view_as_real_copy.out

- func: view_as_complex_copy(Tensor self) -> Tensor
  variants: function
  dispatch:
    CompositeExplicitAutogradNonFunctional: view_as_complex_copy
  tags: view_copy
  autogen: view_as_complex_copy.out

- func: as_strided_copy(Tensor self, SymInt[] size, SymInt[] stride, SymInt? storage_offset=None) -> Tensor
  variants: function
  dispatch:
    CompositeExplicitAutogradNonFunctional: as_strided_copy_symint
  tags: view_copy
  autogen: as_strided_copy.out

- func: as_strided(Tensor(a) self, SymInt[] size, SymInt[] stride, SymInt? storage_offset=None) -> Tensor(a)
  variants: function, method
  dispatch:
    XPU: as_strided_tensorimpl
  device_check: NoCheck
  device_guard: False
  tags: core

- func: _reshape_alias(Tensor(a) self, SymInt[] size, SymInt[] stride) -> Tensor(a)
  variants: function, method
  device_check: NoCheck
  device_guard: False
  dispatch:
    XPU: _reshape_alias

- func: _reshape_alias_copy(Tensor self, SymInt[] size, SymInt[] stride) -> Tensor
  variants: function
  dispatch:
    CompositeExplicitAutogradNonFunctional: _reshape_alias_copy_symint
  tags: view_copy
  autogen: _reshape_alias_copy.out

- func: resize_(Tensor(a!) self, SymInt[] size, *, MemoryFormat? memory_format=None) -> Tensor(a!)
  use_const_ref_for_mutable_tensors: True
  variants: method
  device_check: NoCheck
  device_guard: False
  tags: [core, inplace_view]
  dispatch:
    XPU: resize_xpu_
  autogen: resize, resize.out

- func: set_.source_Storage(Tensor(a!) self, Storage source) -> Tensor(a!)
  variants: method
  device_check: NoCheck
  device_guard: False
  dispatch:
    XPU: set_
  autogen: set.source_Storage, set.source_Storage_out
  tags: inplace_view

- func: set_.source_Storage_storage_offset(Tensor(a!) self, Storage source, SymInt storage_offset, SymInt[] size, SymInt[] stride=[]) -> Tensor(a!)
  variants: method
  device_check: NoCheck
  device_guard: False
  dispatch:
    XPU: set_storage_xpu_
  autogen: set.source_Storage_storage_offset, set.source_Storage_storage_offset_out
  tags: inplace_view

# - func: unfold(Tensor(a) self, int dimension, int size, int step) -> Tensor(a)
#   variants: method
#   device_check: NoCheck
#   device_guard: False
#   dispatch:
#     XPU: unfold

- func: bitwise_and.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
  device_check: NoCheck   # TensorIterator
  structured: True
  structured_inherits: TensorIteratorBase
  variants: function
  dispatch:
    XPU: bitwise_and_out
  tags: pointwise

- func: bitwise_and.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
  device_check: NoCheck   # TensorIterator
  variants: function
  tags: pointwise

- func: bitwise_and.Scalar(Tensor self, Scalar other) -> Tensor
  device_check: NoCheck   # TensorIterator
  variants: method, function
  tags: [core, pointwise]

- func: bitwise_and.Scalar_Tensor(Scalar self, Tensor other) -> Tensor
  device_check: NoCheck   # TensorIterator
  variants: function
  autogen: bitwise_and.Scalar_Tensor_out
  tags: pointwise

- func: bitwise_and.Tensor(Tensor self, Tensor other) -> Tensor
  device_check: NoCheck   # TensorIterator
  variants: method, function
  structured_delegate: bitwise_and.Tensor_out
  tags: [core, pointwise]

- func: bitwise_and_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)
  device_check: NoCheck   # TensorIterator
  variants: method
  tags: pointwise

- func: bitwise_and_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)
  device_check: NoCheck   # TensorIterator
  variants: method
  structured_delegate: bitwise_and.Tensor_out
  tags: pointwise

- func: bitwise_xor.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
  device_check: NoCheck   # TensorIterator
  structured: True
  structured_inherits: TensorIteratorBase
  variants: function
  dispatch:
    XPU: bitwise_xor_out
  tags: pointwise

- func: bitwise_xor.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
  device_check: NoCheck   # TensorIterator
  variants: function
  tags: pointwise

- func: bitwise_xor.Scalar(Tensor self, Scalar other) -> Tensor
  device_check: NoCheck   # TensorIterator
  variants: method, function
  tags: [core, pointwise]

- func: bitwise_xor.Scalar_Tensor(Scalar self, Tensor other) -> Tensor
  device_check: NoCheck   # TensorIterator
  variants: function
  autogen: bitwise_xor.Scalar_Tensor_out
  tags: pointwise

- func: bitwise_xor.Tensor(Tensor self, Tensor other) -> Tensor
  device_check: NoCheck   # TensorIterator
  variants: method, function
  structured_delegate: bitwise_xor.Tensor_out
  tags: [core, pointwise]

- func: bitwise_xor_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)
  device_check: NoCheck   # TensorIterator
  variants: method
  dispatch:
    CompositeExplicitAutograd: bitwise_xor_
  tags: pointwise

- func: bitwise_xor_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)
  device_check: NoCheck   # TensorIterator
  variants: method
  structured_delegate: bitwise_xor.Tensor_out
  tags: pointwise

- func: bitwise_or.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
  device_check: NoCheck   # TensorIterator
  structured: True
  structured_inherits: TensorIteratorBase
  variants: function
  dispatch:
    XPU: bitwise_or_out
  tags: pointwise

- func: bitwise_or.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
  device_check: NoCheck   # TensorIterator
  variants: function
  tags: pointwise

- func: bitwise_or.Scalar(Tensor self, Scalar other) -> Tensor
  device_check: NoCheck   # TensorIterator
  variants: method, function
  tags: [core, pointwise]

- func: bitwise_or.Scalar_Tensor(Scalar self, Tensor other) -> Tensor
  device_check: NoCheck   # TensorIterator
  variants: function
  autogen: bitwise_or.Scalar_Tensor_out
  tags: pointwise

- func: bitwise_or.Tensor(Tensor self, Tensor other) -> Tensor
  device_check: NoCheck   # TensorIterator
  variants: method, function
  structured_delegate: bitwise_or.Tensor_out
  tags: [core, pointwise]

- func: bitwise_or_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)
  device_check: NoCheck   # TensorIterator
  variants: method
  tags: pointwise

- func: bitwise_or_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)
  device_check: NoCheck   # TensorIterator
  variants: method
  structured_delegate: bitwise_or.Tensor_out
  tags: pointwise

# - func: bitwise_not(Tensor self) -> Tensor
#   device_check: NoCheck   # TensorIterator
#   structured_delegate: bitwise_not.out
#   variants: function, method
#   tags: [core, pointwise]

# - func: bitwise_not_(Tensor(a!) self) -> Tensor(a!)
#   device_check: NoCheck   # TensorIterator
#   structured_delegate: bitwise_not.out
#   variants: method
#   tags: pointwise

# - func: bitwise_not.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
#   device_check: NoCheck   # TensorIterator
#   structured: True
#   structured_inherits: TensorIteratorBase
#   dispatch:
#     XPU: bitwise_not_out
#   tags: pointwise

- func: where.self_out(Tensor condition, Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
  device_check: NoCheck   # TensorIterator
  dispatch:
    XPU: where_self_out

- func: where.self(Tensor condition, Tensor self, Tensor other) -> Tensor
  device_check: NoCheck   # TensorIterator
  variants: function, method
  dispatch:
    XPU: where
  tags: [core, pointwise]

- func: clamp(Tensor self, Scalar? min=None, Scalar? max=None) -> Tensor
  device_check: NoCheck   # TensorIterator
  variants: function, method
  cpp_no_default_args: ['min']
  structured_delegate: clamp.out
  tags: [core, pointwise]

- func: clamp.Tensor(Tensor self, Tensor? min=None, Tensor? max=None) -> Tensor
  variants: function, method
  structured_delegate: clamp.Tensor_out
  tags: [core, pointwise]

- func: clamp_(Tensor(a!) self, Scalar? min=None, Scalar? max=None) -> Tensor(a!)
  device_check: NoCheck   # TensorIterator
  variants: function, method
  cpp_no_default_args: ['min']
  structured_delegate: clamp.out
  tags: pointwise

- func: clamp_.Tensor(Tensor(a!) self, Tensor? min=None, Tensor? max=None) -> Tensor(a!)
  variants: function, method
  structured_delegate: clamp.Tensor_out
  tags: pointwise

- func: clamp.out(Tensor self, Scalar? min=None, Scalar? max=None, *, Tensor(a!) out) -> Tensor(a!)
  device_check: NoCheck   # TensorIterator
  cpp_no_default_args: ['min']
  structured: True
  structured_inherits: TensorIteratorBase
  dispatch:
    XPU: clamp_out
  tags: pointwise

- func: clamp.Tensor_out(Tensor self, Tensor? min=None, Tensor? max=None, *, Tensor(a!) out) -> Tensor(a!)
  device_check: NoCheck   # TensorIterator
  structured: True
  structured_inherits: TensorIteratorBase
  dispatch:
    CPU, CUDA: clamp_Tensor_out
    MPS: clamp_Tensor_out_mps
  tags: pointwise

- func: clamp_max(Tensor self, Scalar max) -> Tensor
  device_check: NoCheck   # TensorIterator
  variants: function, method
  structured_delegate: clamp_max.out
  tags: pointwise

- func: clamp_max.Tensor(Tensor self, Tensor max) -> Tensor
  variants: function, method
  structured_delegate: clamp_max.Tensor_out
  tags: pointwise

- func: clamp_max_(Tensor(a!) self, Scalar max) -> Tensor(a!)
  device_check: NoCheck   # TensorIterator
  variants: function, method
  structured_delegate: clamp_max.out
  tags: pointwise

- func: clamp_max_.Tensor(Tensor(a!) self, Tensor max) -> Tensor(a!)
  variants: function, method
  structured_delegate: clamp_max.Tensor_out
  tags: pointwise

- func: clamp_max.out(Tensor self, Scalar max, *, Tensor(a!) out) -> Tensor(a!)
  device_check: NoCheck   # TensorIterator
  structured: True
  structured_inherits: TensorIteratorBase
  dispatch:
    XPU: clamp_max_out
  tags: pointwise

- func: clamp_max.Tensor_out(Tensor self, Tensor max, *, Tensor(a!) out) -> Tensor(a!)
  device_check: NoCheck   # TensorIterator
  structured: True
  structured_inherits: TensorIteratorBase
  dispatch:
    XPU: clamp_max_Tensor_out
  tags: pointwise

- func: clamp_min(Tensor self, Scalar min) -> Tensor
  device_check: NoCheck   # TensorIterator
  variants: function, method
  structured_delegate: clamp_min.out
  tags: pointwise

- func: clamp_min.Tensor(Tensor self, Tensor min) -> Tensor
  variants: function, method
  structured_delegate: clamp_min.Tensor_out
  tags: pointwise

- func: clamp_min_(Tensor(a!) self, Scalar min) -> Tensor(a!)
  device_check: NoCheck   # TensorIterator
  variants: function, method
  structured_delegate: clamp_min.out
  tags: pointwise

- func: clamp_min_.Tensor(Tensor(a!) self, Tensor min) -> Tensor(a!)
  variants: function, method
  structured_delegate: clamp_min.Tensor_out
  tags: pointwise

- func: clamp_min.out(Tensor self, Scalar min, *, Tensor(a!) out) -> Tensor(a!)
  device_check: NoCheck   # TensorIterator
  structured: True
  structured_inherits: TensorIteratorBase
  dispatch:
    XPU: clamp_min_out
  tags: pointwise

- func: clamp_min.Tensor_out(Tensor self, Tensor min, *, Tensor(a!) out) -> Tensor(a!)
  device_check: NoCheck   # TensorIterator
  structured: True
  structured_inherits: TensorIteratorBase
  dispatch:
    XPU: clamp_min_Tensor_out
  tags: pointwise

- func: max(Tensor self) -> Tensor
  device_check: NoCheck   # TensorIterator
  variants: method, function
  dispatch:
    XPU: max

- func: max.unary_out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
  device_check: NoCheck   # TensorIterator
  dispatch:
    XPU: max_unary_out

- func: max.dim(Tensor self, int dim, bool keepdim=False) -> (Tensor values, Tensor indices)
  device_check: NoCheck   # TensorIterator
  structured_delegate: max.dim_max
  variants: function, method
  tags: core

- func: max.dim_max(Tensor self, int dim, bool keepdim=False, *, Tensor(a!) max, Tensor(b!) max_values) -> (Tensor(a!) values, Tensor(b!) indices)
  device_check: NoCheck   # TensorIterator
  structured: True
  precomputed:
  - dim -> int dim
  dispatch:
    XPU: max_out

- func: max.names_dim(Tensor self, Dimname dim, bool keepdim=False) -> (Tensor values, Tensor indices)
  device_check: NoCheck   # TensorIterator
  variants: function, method

- func: max.names_dim_max(Tensor self, Dimname dim, bool keepdim=False, *, Tensor(a!) max, Tensor(b!) max_values) -> (Tensor(a!) values, Tensor(b!) indices)
  device_check: NoCheck   # TensorIterator


- func: min(Tensor self) -> Tensor
  device_check: NoCheck   # TensorIterator
  variants: method, function
  dispatch:
    XPU: min

- func: min.dim(Tensor self, int dim, bool keepdim=False) -> (Tensor values, Tensor indices)
  device_check: NoCheck   # TensorIterator
  structured_delegate: min.dim_min
  variants: function, method
  dispatch:
    QuantizedCPU, QuantizedCUDA: qmin
  tags: core

- func: min.dim_min(Tensor self, int dim, bool keepdim=False, *, Tensor(a!) min, Tensor(b!) min_indices) -> (Tensor(a!) values, Tensor(b!) indices)
  device_check: NoCheck   # TensorIterator
  structured: True
  precomputed:
  - dim -> int dim
  dispatch:
    XPU: min_out

- func: min.names_dim(Tensor self, Dimname dim, bool keepdim=False) -> (Tensor values, Tensor indices)
  device_check: NoCheck   # TensorIterator
  variants: function, method

- func: min.names_dim_min(Tensor self, Dimname dim, bool keepdim=False, *, Tensor(a!) min, Tensor(b!) min_indices) -> (Tensor(a!) values, Tensor(b!) indices)
  device_check: NoCheck   # TensorIterator

# - func: sum.IntList_out(Tensor self, int[1]? dim, bool keepdim=False, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)
#   structured: True
#   device_check: NoCheck   # TensorIterator
#   dispatch:
#     XPU: sum_out

# - func: mean.out(Tensor self, int[1]? dim, bool keepdim=False, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)
#   structured: True
#   device_check: NoCheck   # TensorIterator
#   dispatch:
#     XPU: mean_out

# - func: any.out(Tensor self, int dim, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)
#   device_check: NoCheck   # TensorIterator
#   structured: True
#   dispatch:
#     XPU: any_out

# - func: any.all_out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
#   device_check: NoCheck
#   structured: True
#   dispatch:
#     XPU: any_all_out

# - func: argmax.out(Tensor self, int? dim=None, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)
#   structured: True
#   dispatch:
#     XPU: argmax_out

- func: _local_scalar_dense(Tensor self) -> Scalar
  tags: [core, data_dependent_output]
  dispatch:
    XPU: _local_scalar_dense_xpu
  variants: function

- func: col2im.out(Tensor self, SymInt[2] output_size, int[2] kernel_size, int[2] dilation, int[2] padding, int[2] stride, *, Tensor(a!) out) -> Tensor(a!)
  python_module: nn
  dispatch:
    XPU: col2im_out_xpu

- func: col2im(Tensor self, SymInt[2] output_size, int[2] kernel_size, int[2] dilation, int[2] padding, int[2] stride) -> Tensor
  python_module: nn
  dispatch:
    XPU: col2im_xpu
  tags: core

- func: im2col.out(Tensor self, int[2] kernel_size, int[2] dilation, int[2] padding, int[2] stride, *, Tensor(a!) out) -> Tensor(a!)
  python_module: nn
  dispatch:
    XPU: im2col_out_xpu

- func: im2col(Tensor self, int[2] kernel_size, int[2] dilation, int[2] padding, int[2] stride) -> Tensor
  python_module: nn
  dispatch:
    XPU: im2col_xpu

- func: flip(Tensor self, int[] dims) -> Tensor
  variants: function, method
  dispatch:
    XPU: flip
  autogen: flip.out
  tags: core

- func: nonzero.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
  dispatch:
    XPU: nonzero_out_xpu
  tags: dynamic_output_shape

- func: nonzero(Tensor self) -> Tensor
  variants: method, function
  dispatch:
    XPU: nonzero_xpu
  tags: [dynamic_output_shape, core]

- func: maximum(Tensor self, Tensor other) -> Tensor
  structured_delegate: maximum.out
  device_check: NoCheck   # TensorIterator
  variants: method, function
  tags: [core, pointwise]

- func: maximum.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
  structured: True
  structured_inherits: TensorIteratorBase
  device_check: NoCheck   # TensorIterator
  dispatch:
    XPU: maximum_out
  tags: pointwise

- func: minimum(Tensor self, Tensor other) -> Tensor
  structured_delegate: minimum.out
  device_check: NoCheck   # TensorIterator
  variants: method, function
  tags: [core, pointwise]

- func: minimum.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
  structured: True
  structured_inherits: TensorIteratorBase
  device_check: NoCheck   # TensorIterator
  dispatch:
    XPU: minimum_out
  tags: pointwise

- func: sigmoid_backward.grad_input(Tensor grad_output, Tensor output, *, Tensor(a!) grad_input) -> Tensor(a!)
  python_module: nn
  structured: True
  structured_inherits: TensorIteratorBase
  dispatch:
    XPU: sigmoid_backward_out
  tags: pointwise

- func: sigmoid_backward(Tensor grad_output, Tensor output) -> Tensor
  python_module: nn
  structured_delegate: sigmoid_backward.grad_input
  tags: pointwise

- func: _softmax(Tensor self, int dim, bool half_to_float) -> Tensor
  structured_delegate: _softmax.out
  tags: core

- func: _softmax.out(Tensor self, int dim, bool half_to_float, *, Tensor(a!) out) -> Tensor(a!)
  structured: True
  dispatch:
    XPU: softmax_xpu_out

- func: _softmax_backward_data(Tensor grad_output, Tensor output, int dim, ScalarType input_dtype) -> Tensor
  structured_delegate: _softmax_backward_data.out

- func: _softmax_backward_data.out(Tensor grad_output, Tensor output, int dim, ScalarType input_dtype, *, Tensor(a!) grad_input) -> Tensor(a!)
  structured: True
  dispatch:
    XPU: softmax_backward_xpu_out

- func: _log_softmax_backward_data(Tensor grad_output, Tensor output, int dim, ScalarType input_dtype) -> Tensor
  structured_delegate: _log_softmax_backward_data.out

- func: _log_softmax_backward_data.out(Tensor grad_output, Tensor output, int dim, ScalarType input_dtype, *, Tensor(a!) out) -> Tensor(a!)
  structured: True
  dispatch:
    XPU: log_softmax_backward_xpu_out

- func: exp(Tensor self) -> Tensor
  device_check: NoCheck   # TensorIterator
  structured_delegate: exp.out
  variants: function, method
  tags: [core, pointwise]

- func: exp_(Tensor(a!) self) -> Tensor(a!)
  device_check: NoCheck   # TensorIterator
  structured_delegate: exp.out
  variants: function, method
  tags: pointwise

- func: exp.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
  device_check: NoCheck   # TensorIterator
  structured: True
  structured_inherits: TensorIteratorBase
  dispatch:
    XPU: exp_out
  tags: pointwise

- func: sigmoid(Tensor self) -> Tensor
  device_check: NoCheck   # TensorIterator
  structured_delegate: sigmoid.out
  variants: function, method
  tags: [core, pointwise]

- func: sigmoid_(Tensor(a!) self) -> Tensor(a!)
  device_check: NoCheck   # TensorIterator
  structured_delegate: sigmoid.out
  variants: function, method
  tags: pointwise

- func: sigmoid.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
  device_check: NoCheck   # TensorIterator
  structured: True
  structured_inherits: TensorIteratorBase
  dispatch:
    XPU: sigmoid_out
  tags: pointwise

- func: sgn(Tensor self) -> Tensor
  variants: function, method
  structured_delegate: sgn.out
  tags: pointwise

- func: sgn_(Tensor(a!) self) -> Tensor(a!)
  variants: method
  structured_delegate: sgn.out
  tags: pointwise

- func: sgn.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
  structured: True
  structured_inherits: TensorIteratorBase
  dispatch:
    XPU: sgn_out
  tags: pointwise

- func: _foreach_add.List(Tensor[] self, Tensor[] other, *, Scalar alpha=1) -> Tensor[]
  device_check: NoCheck   # foreach kernels fall back to slow path when tensor are on different devices
  variants: function
  dispatch:
    XPU: foreach_tensor_add_list_kernel_xpu

- func: _foreach_add_.List(Tensor(a!)[] self, Tensor[] other, *, Scalar alpha=1) -> ()
  device_check: NoCheck   # foreach kernels fall back to slow path when tensor are on different devices
  variants: function
  dispatch:
    XPU: foreach_tensor_add_list_kernel_xpu_
  autogen: _foreach_add.List_out

- func: _foreach_add.Scalar(Tensor[] self, Scalar scalar) -> Tensor[]
  device_check: NoCheck   # foreach kernels fall back to slow path when tensor are on different devices
  variants: function
  dispatch:
    XPU: foreach_tensor_add_scalar_kernel_xpu

- func: _foreach_add_.Scalar(Tensor(a!)[] self, Scalar scalar) -> ()
  device_check: NoCheck   # foreach kernels fall back to slow path when tensor are on different devices
  variants: function
  dispatch:
    XPU: foreach_tensor_add_scalar_kernel_xpu_
  autogen: _foreach_add.Scalar_out

- func: _foreach_mul.Scalar(Tensor[] self, Scalar scalar) -> Tensor[]
  device_check: NoCheck   # foreach kernels fall back to slow path when tensor are on different devices
  variants: function
  dispatch:
    XPU: foreach_tensor_mul_scalar_kernel_xpu

- func: _foreach_mul.List(Tensor[] self, Tensor[] other) -> Tensor[]
  device_check: NoCheck   # foreach kernels fall back to slow path when tensor are on different devices
  variants: function
  dispatch:
    XPU: foreach_tensor_mul_list_kernel_xpu

- func: _foreach_mul_.List(Tensor(a!)[] self, Tensor[] other) -> ()
  device_check: NoCheck   # foreach kernels fall back to slow path when tensor are on different devices
  variants: function
  dispatch:
    XPU: foreach_tensor_mul_list_kernel_xpu_
  autogen: _foreach_mul.List_out

- func: _foreach_mul_.Scalar(Tensor(a!)[] self, Scalar scalar) -> ()
  device_check: NoCheck   # foreach kernels fall back to slow path when tensor are on different devices
  variants: function
  dispatch:
    XPU: foreach_tensor_mul_scalar_kernel_xpu_
  autogen: _foreach_mul.Scalar_out

- func: _foreach_div.List(Tensor[] self, Tensor[] other) -> Tensor[]
  device_check: NoCheck   # foreach kernels fall back to slow path when tensor are on different devices
  variants: function
  dispatch:
    XPU: foreach_tensor_div_list_kernel_xpu

- func: _foreach_div_.List(Tensor(a!)[] self, Tensor[] other) -> ()
  device_check: NoCheck   # foreach kernels fall back to slow path when tensor are on different devices
  variants: function
  dispatch:
    XPU: foreach_tensor_div_list_kernel_xpu_
  autogen: _foreach_div.List_out

- func: _foreach_div.Scalar(Tensor[] self, Scalar scalar) -> Tensor[]
  device_check: NoCheck   # foreach kernels fall back to slow path when tensor are on different devices
  variants: function
  dispatch:
    XPU: foreach_tensor_div_scalar_kernel_xpu

- func: _foreach_div_.Scalar(Tensor(a!)[] self, Scalar scalar) -> ()
  device_check: NoCheck   # foreach kernels fall back to slow path when tensor are on different devices
  variants: function
  dispatch:
    XPU: foreach_tensor_div_scalar_kernel_xpu_
  autogen: _foreach_div.Scalar_out

- func: _foreach_addcmul.Scalar(Tensor[] self, Tensor[] tensor1, Tensor[] tensor2, Scalar value=1) -> Tensor[]
  device_check: NoCheck   # foreach kernels fall back to slow path when tensor are on different devices
  variants: function
  dispatch:
    XPU: foreach_tensor_addcmul_scalar_xpu

- func: _foreach_addcmul_.Scalar(Tensor(a!)[] self, Tensor[] tensor1, Tensor[] tensor2, Scalar value=1) -> ()
  device_check: NoCheck   # foreach kernels fall back to slow path when tensor are on different devices
  variants: function
  dispatch:
    XPU: foreach_tensor_addcmul_scalar_xpu_
  autogen: _foreach_addcmul.Scalar_out


- func: _foreach_addcmul.ScalarList(Tensor[] self, Tensor[] tensor1, Tensor[] tensor2, Scalar[] scalars) -> Tensor[]
  device_check: NoCheck   # foreach kernels fall back to slow path when tensor are on different devices
  variants: function
  dispatch:
    XPU: foreach_tensor_addcmul_scalarlist_xpu

- func: _foreach_addcmul_.ScalarList(Tensor(a!)[] self, Tensor[] tensor1, Tensor[] tensor2, Scalar[] scalars) -> ()
  device_check: NoCheck   # foreach kernels fall back to slow path when tensor are on different devices
  variants: function
  dispatch:
    XPU: foreach_tensor_addcmul_scalarlist_xpu_
  autogen: _foreach_addcmul.ScalarList_out

- func: _foreach_addcdiv_.ScalarList(Tensor(a!)[] self, Tensor[] tensor1, Tensor[] tensor2, Scalar[] scalars) -> ()
  device_check: NoCheck   # foreach kernels fall back to slow path when tensor are on different devices
  variants: function
  dispatch:
    XPU: foreach_tensor_addcdiv_scalarlist_xpu_
  autogen: _foreach_addcdiv.ScalarList_out

- func: _foreach_addcdiv.ScalarList(Tensor[] self, Tensor[] tensor1, Tensor[] tensor2, Scalar[] scalars) -> Tensor[]
  device_check: NoCheck   # foreach kernels fall back to slow path when tensor are on different devices
  variants: function
  dispatch:
    XPU: foreach_tensor_addcdiv_scalarlist_xpu

- func: _foreach_addcdiv.Scalar(Tensor[] self, Tensor[] tensor1, Tensor[] tensor2, Scalar value=1) -> Tensor[]
  device_check: NoCheck   # foreach kernels fall back to slow path when tensor are on different devices
  variants: function
  dispatch:
    XPU: foreach_tensor_addcdiv_scalar_xpu

- func: _foreach_addcdiv_.Scalar(Tensor(a!)[] self, Tensor[] tensor1, Tensor[] tensor2, Scalar value=1) -> ()
  device_check: NoCheck   # foreach kernels fall back to slow path when tensor are on different devices
  variants: function
  dispatch:
    XPU: foreach_tensor_addcdiv_scalar_xpu_
  autogen: _foreach_addcdiv.Scalar_out

- func: native_layer_norm(Tensor input, SymInt[] normalized_shape, Tensor? weight, Tensor? bias, float eps) -> (Tensor, Tensor, Tensor)
  dispatch:
    XPU: layer_norm_xpu
  autogen: native_layer_norm.out
  tags: core

- func: native_layer_norm_backward(Tensor grad_out, Tensor input, SymInt[] normalized_shape, Tensor mean, Tensor rstd, Tensor? weight, Tensor? bias, bool[3] output_mask) -> (Tensor, Tensor, Tensor)
  dispatch:
    XPU: layer_norm_backward_xpu
  autogen: native_layer_norm_backward.out
  tags: core

- func: tril.out(Tensor self, int diagonal=0, *, Tensor(a!) out) -> Tensor(a!)
  structured: True
  dispatch:
    XPU: tril_xpu

- func: tril(Tensor self, int diagonal=0) -> Tensor
  structured_delegate: tril.out
  variants: method, function

- func: tril_(Tensor(a!) self, int diagonal=0) -> Tensor(a!)
  structured_delegate: tril.out
  variants: method

- func: triu_(Tensor(a!) self, int diagonal=0) -> Tensor(a!)
  structured_delegate: triu.out
  variants: method

- func: triu.out(Tensor self, int diagonal=0, *, Tensor(a!) out) -> Tensor(a!)
  structured: True
  dispatch:
    XPU: triu_xpu

- func: triu(Tensor self, int diagonal=0) -> Tensor
  structured_delegate: triu.out
  variants: method, function

- func: set_(Tensor(a!) self) -> Tensor(a!)
  variants: method
  dispatch:
    XPU: set_xpu_
  autogen: set, set.out
  tags: inplace_view

- func: set_.source_Tensor(Tensor(a!) self, Tensor source) -> Tensor(a!)
  variants: method
  device_check: NoCheck
  device_guard: False
  dispatch:
    XPU: set_tensor_
  autogen: set.source_Tensor, set.source_Tensor_out
  tags: inplace_view

- func: nll_loss_forward.output(Tensor self, Tensor target, Tensor? weight, int reduction, SymInt ignore_index, *, Tensor(a!) output, Tensor(b!) total_weight) -> (Tensor(a!), Tensor(b!))
  python_module: nn
  structured: True
  dispatch:
    XPU: nll_loss_forward_out_xpu

- func: nll_loss_forward(Tensor self, Tensor target, Tensor? weight, int reduction, SymInt ignore_index) -> (Tensor output, Tensor total_weight)
  python_module: nn
  structured_delegate: nll_loss_forward.output

- func: nll_loss_backward.grad_input(Tensor grad_output, Tensor self, Tensor target, Tensor? weight, int reduction, SymInt ignore_index, Tensor total_weight, *, Tensor(a!) grad_input) -> Tensor(a!)
  python_module: nn
  structured: True
  dispatch:
    XPU: nll_loss_backward_out_xpu

- func: nll_loss_backward(Tensor grad_output, Tensor self, Tensor target, Tensor? weight, int reduction, SymInt ignore_index, Tensor total_weight) -> Tensor
  python_module: nn
  structured_delegate: nll_loss_backward.grad_input

# - func: sort.values(Tensor self, int dim=-1, bool descending=False, *, Tensor(a!) values, Tensor(b!) indices) -> (Tensor(a!) values, Tensor(b!) indices)
#   device_check: NoCheck   # TensorIterator
#   dispatch:
#     CompositeExplicitAutograd: sort_out

# - func: sort.values_stable(Tensor self, *, bool? stable, int dim=-1, bool descending=False, Tensor(a!) values, Tensor(b!) indices) -> (Tensor(a!) values, Tensor(b!) indices)
#   structured: True
#   dispatch:
#     XPU: sort_stable_out

# - func: sort(Tensor self, int dim=-1, bool descending=False) -> (Tensor values, Tensor indices)
#   device_check: NoCheck   # TensorIterator
#   variants: method, function
#   dispatch:
#     CompositeExplicitAutograd: sort
#   tags: core

# - func: sort.stable(Tensor self, *, bool? stable, int dim=-1, bool descending=False) -> (Tensor values, Tensor indices)
#   structured_delegate: sort.values_stable
#   variants: method, function

- func: cat(Tensor[] tensors, int dim=0) -> Tensor
  structured_delegate: cat.out
  tags: core

- func: cat.out(Tensor[] tensors, int dim=0, *, Tensor(a!) out) -> Tensor(a!)
  structured: True
  precomputed:
  - dim -> int dim, int valid, bool all_contiguous, bool all_same_dtype, bool all_same_sizes_and_stride, MemoryFormat memory_format
  dispatch:
    XPU: cat_out_xpu

- func: is_pinned(Tensor self, Device? device=None) -> bool
  variants: method
  dispatch:
    XPU: is_pinned_xpu

# TODO: add a copy kwarg that guarantees that the tensor is put into fresh
# pinned memory
- func: pin_memory(Tensor(a) self, Device? device=None) -> Tensor(a)
  variants: method

# Unlike pin_memory, this is guaranteed to give a new non-aliasing tensor
- func: _pin_memory(Tensor self, Device? device=None) -> Tensor
  dispatch:
    XPU: _pin_memory_xpu
  autogen: _pin_memory.out

- func: _embedding_bag_forward_only(Tensor weight, Tensor indices, Tensor offsets, bool scale_grad_by_freq=False, int mode=0, bool sparse=False, Tensor? per_sample_weights=None, bool include_last_offset=False, int padding_idx=-1) -> (Tensor, Tensor, Tensor, Tensor)
  dispatch:
    XPU: _embedding_bag_forward_only_xpu
  autogen: _embedding_bag_forward_only.out

- func: _embedding_bag(Tensor weight, Tensor indices, Tensor offsets, bool scale_grad_by_freq=False, int mode=0, bool sparse=False, Tensor? per_sample_weights=None, bool include_last_offset=False, int padding_idx=-1) -> (Tensor, Tensor, Tensor, Tensor)
  dispatch:
    XPU: _embedding_bag_xpu
  autogen: _embedding_bag.out
  tags: core

- func: max_pool2d_with_indices_backward.grad_input(Tensor grad_output, Tensor self, int[2] kernel_size, int[2] stride, int[2] padding, int[2] dilation, bool ceil_mode, Tensor indices, *, Tensor(a!) grad_input) -> Tensor(a!)
  python_module: nn
  structured: True
  dispatch:
    XPU: max_pool2d_with_indices_backward_out_xpu

- func: max_pool2d_with_indices_backward(Tensor grad_output, Tensor self, int[2] kernel_size, int[2] stride, int[2] padding, int[2] dilation, bool ceil_mode, Tensor indices) -> Tensor
  python_module: nn
  structured_delegate: max_pool2d_with_indices_backward.grad_input
  tags: core

- func: _adaptive_avg_pool2d_backward(Tensor grad_output, Tensor self) -> Tensor
  python_module: nn
  dispatch:
    XPU: adaptive_avg_pool2d_backward_xpu
  autogen: _adaptive_avg_pool2d_backward.out
  tags: core

- func: embedding_dense_backward(Tensor grad_output, Tensor indices, SymInt num_weights, SymInt padding_idx, bool scale_grad_by_freq) -> Tensor
  dispatch:
    XPU: embedding_dense_backward_xpu
  autogen: embedding_dense_backward.out
  tags: core

- func: elu.out(Tensor self, Scalar alpha=1, Scalar scale=1, Scalar input_scale=1, *, Tensor(a!) out) -> Tensor(a!)
  structured: True
  structured_inherits: TensorIteratorBase
  device_check: NoCheck   # TensorIterator
  python_module: nn
  dispatch:
    XPU: elu_out

- func: elu(Tensor self, Scalar alpha=1, Scalar scale=1, Scalar input_scale=1) -> Tensor
  structured_delegate: elu.out
  device_check: NoCheck   # TensorIterator
  python_module: nn

- func: elu_backward.grad_input(Tensor grad_output, Scalar alpha, Scalar scale, Scalar input_scale, bool is_result, Tensor self_or_result, *, Tensor(a!) grad_input) -> Tensor(a!)
  structured: True
  structured_inherits: TensorIteratorBase
  python_module: nn
  dispatch:
    XPU: elu_backward_out

- func: elu_backward(Tensor grad_output, Scalar alpha, Scalar scale, Scalar input_scale, bool is_result, Tensor self_or_result) -> Tensor
  structured_delegate: elu_backward.grad_input
  python_module: nn

- func: elu_(Tensor(a!) self, Scalar alpha=1, Scalar scale=1, Scalar input_scale=1) -> Tensor(a!)
  structured_delegate: elu.out
  device_check: NoCheck   # TensorIterator
  python_module: nn

- func: silu(Tensor self) -> Tensor
  structured_delegate: silu.out
  python_module: nn
  tags: pointwise

- func: silu_(Tensor(a!) self) -> Tensor(a!)
  structured_delegate: silu.out
  python_module: nn
  tags: pointwise

- func: silu.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
  structured: True
  structured_inherits: TensorIteratorBase
  python_module: nn
  dispatch:
    XPU: silu_out
  tags: pointwise

- func: silu_backward.grad_input(Tensor grad_output, Tensor self, *, Tensor(a!) grad_input) -> Tensor(a!)
  structured: True
  structured_inherits: TensorIteratorBase
  python_module: nn
  dispatch:
    XPU: silu_backward_out
  tags: pointwise

- func: silu_backward(Tensor grad_output, Tensor self) -> Tensor
  structured_delegate: silu_backward.grad_input
  python_module: nn
  dispatch:
    CompositeImplicitAutograd: math_silu_backward
  tags: pointwise

- func: hardswish.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
  device_check: NoCheck   # TensorIterator
  python_module: nn
  dispatch:
    XPU: hardswish_out

- func: hardswish(Tensor self) -> Tensor
  device_check: NoCheck   # TensorIterator
  python_module: nn
  dispatch:
    XPU: hardswish

- func: hardswish_(Tensor(a!) self) -> Tensor(a!)
  device_check: NoCheck   # TensorIterator
  python_module: nn
  dispatch:
    XPU: hardswish_

- func: hardswish_backward(Tensor grad_output, Tensor self) -> Tensor
  python_module: nn
  dispatch:
    XPU: hardswish_backward
  autogen: hardswish_backward.out

- func: hardtanh.out(Tensor self, Scalar min_val=-1, Scalar max_val=1, *, Tensor(a!) out) -> Tensor(a!)
  device_check: NoCheck   # TensorIterator
  python_module: nn
  dispatch:
    XPU: hardtanh_out

- func: hardtanh(Tensor self, Scalar min_val=-1, Scalar max_val=1) -> Tensor
  device_check: NoCheck   # TensorIterator
  python_module: nn
  dispatch:
    XPU: hardtanh
  tags: core

- func: hardtanh_backward.grad_input(Tensor grad_output, Tensor self, Scalar min_val, Scalar max_val, *, Tensor(a!) grad_input) -> Tensor(a!)
  python_module: nn
  dispatch:
    XPU: hardtanh_backward_out
    MPS: hardtanh_backward_out_mps

- func: hardtanh_backward(Tensor grad_output, Tensor self, Scalar min_val, Scalar max_val) -> Tensor
  python_module: nn
  dispatch:
    XPU: hardtanh_backward

- func: hardtanh_(Tensor(a!) self, Scalar min_val=-1, Scalar max_val=1) -> Tensor(a!)
  device_check: NoCheck   # TensorIterator
  python_module: nn
  dispatch:
    XPU: hardtanh_

- func: relu(Tensor self) -> Tensor
  device_check: NoCheck   # TensorIterator
  variants: function, method
  dispatch:
    XPU: relu
  tags: [core, pointwise]

- func: relu_(Tensor(a!) self) -> Tensor(a!)
  device_check: NoCheck   # TensorIterator
  variants: function, method
  dispatch:
    XPU: relu_
  autogen: relu.out
  tags: pointwise

- func: all.dim(Tensor self, int dim, bool keepdim=False) -> Tensor
  device_check: NoCheck   # TensorIterator
  structured_delegate: all.out
  variants: function, method

- func: all.dims(Tensor self, int[]? dim=None, bool keepdim=False) -> Tensor
  device_check: NoCheck   # TensorIterator
  structured_delegate: all.dims_out
  variants: function, method
  cpp_no_default_args: ['dim']
  dispatch:
    CompositeExplicitAutograd: all_dims_default

- func: all.out(Tensor self, int dim, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)
  device_check: NoCheck   # TensorIterator
  structured: True
  dispatch:
    XPU: all_out

- func: all.dims_out(Tensor self, int[]? dim=None, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)
  device_check: NoCheck   # TensorIterator
  structured: True
  dispatch:
    XPU: all_dims_out
    CompositeExplicitAutograd: all_dims_out_default
  cpp_no_default_args: ['dim']

- func: all.dimname(Tensor self, Dimname dim, bool keepdim=False) -> Tensor
  device_check: NoCheck   # TensorIterator
  variants: function, method

- func: all.dimname_out(Tensor self, Dimname dim, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)
  device_check: NoCheck   # TensorIterator

- func: all(Tensor self) -> Tensor
  device_check: NoCheck   # TensorIterator
  structured_delegate: all.all_out
  variants: method, function

- func: all.all_out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
  device_check: NoCheck
  structured: True
  dispatch:
    XPU: all_all_out

- func: any.dim(Tensor self, int dim, bool keepdim=False) -> Tensor
  device_check: NoCheck   # TensorIterator
  structured_delegate: any.out
  variants: function, method
  tags: core

- func: any.dims(Tensor self, int[]? dim=None, bool keepdim=False) -> Tensor
  device_check: NoCheck   # TensorIterator
  structured_delegate: any.dims_out
  variants: function, method
  cpp_no_default_args: ['dim']
  tags: core
  dispatch:
    CompositeExplicitAutograd: any_dims_default

- func: any.out(Tensor self, int dim, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)
  device_check: NoCheck   # TensorIterator
  structured: True
  dispatch:
    XPU: any_out

- func: any.dims_out(Tensor self, int[]? dim=None, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)
  device_check: NoCheck   # TensorIterator
  structured: True
  dispatch:
    XPU: any_dims_out
    CompositeExplicitAutograd: any_dims_out_default
  cpp_no_default_args: ['dim']

- func: any.dimname(Tensor self, Dimname dim, bool keepdim=False) -> Tensor
  device_check: NoCheck   # TensorIterator
  variants: function, method

- func: any.dimname_out(Tensor self, Dimname dim, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)
  device_check: NoCheck   # TensorIterator

- func: any(Tensor self) -> Tensor
  device_check: NoCheck   # TensorIterator
  structured_delegate: any.all_out
  variants: method, function
  tags: core

- func: any.all_out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
  device_check: NoCheck
  structured: True
  dispatch:
    XPU: any_all_out

- func: addr(Tensor self, Tensor vec1, Tensor vec2, *, Scalar beta=1, Scalar alpha=1) -> Tensor
  variants: function, method
  dispatch:
    XPU: addr

- func: addr_(Tensor(a!) self, Tensor vec1, Tensor vec2, *, Scalar beta=1, Scalar alpha=1) -> Tensor(a!)
  variants: method

- func: addr.out(Tensor self, Tensor vec1, Tensor vec2, *, Scalar beta=1, Scalar alpha=1, Tensor(a!) out) -> Tensor(a!)
  dispatch:
    XPU: addr_out

- func: _foreach_sqrt(Tensor[] self) -> Tensor[]
  device_check: NoCheck   # foreach kernels fall back to slow path when tensor are on different devices
  variants: function
  dispatch:
    XPU: foreach_tensor_sqrt_xpu

- func: _foreach_sqrt_(Tensor(a!)[] self) -> ()
  device_check: NoCheck   # foreach kernels fall back to slow path when tensor are on different devices
  variants: function
  dispatch:
    XPU: foreach_tensor_sqrt_xpu_
  autogen: _foreach_sqrt.out

- func: _foreach_lerp.List(Tensor[] self, Tensor[] tensors1, Tensor[] weights) -> Tensor[]
  device_check: NoCheck   # foreach kernels fall back to slow path when tensors are on different devices
  variants: function
  dispatch:
    XPU: foreach_tensor_lerp_ternary_xpu
  autogen: _foreach_lerp.List_out

- func: _foreach_lerp_.List(Tensor(a!)[] self, Tensor[] tensors1, Tensor[] weights) -> ()
  device_check: NoCheck   # foreach kernels fall back to slow path when tensors are on different devices
  variants: function
  dispatch:
    XPU: foreach_tensor_lerp_ternary_xpu_
  autogen: _foreach_lerp.List_out

- func: _foreach_lerp.Scalar(Tensor[] self, Tensor[] tensors1, Scalar weight) -> Tensor[]
  device_check: NoCheck   # foreach kernels fall back to slow path when tensors are on different devices
  variants: function
  dispatch:
    XPU: foreach_tensor_lerp_list_xpu
  autogen: _foreach_lerp.Scalar_out

- func: _foreach_lerp_.Scalar(Tensor(a!)[] self, Tensor[] tensors1, Scalar weight) -> ()
  device_check: NoCheck   # foreach kernels fall back to slow path when tensors are on different devices
  variants: function
  dispatch:
    XPU: foreach_tensor_lerp_list_xpu_
  autogen: _foreach_lerp.Scalar_out